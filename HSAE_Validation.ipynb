{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('protein_expression.csv')\n",
    "inputed_columns = ['AGID00215',\n",
    " 'AGID00537',\n",
    " 'AGID00536',\n",
    " 'AGID00211',\n",
    " 'AGID00485',\n",
    " 'AGID00383',\n",
    " 'AGID00216',\n",
    " 'AGID00257',\n",
    " 'AGID00545',\n",
    " 'AGID00413',\n",
    " 'AGID00547',\n",
    " 'AGID00144']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_columns = merged_df.columns.drop([\"ajcc_pathologic_stage\",\"vital_status\",\"days_to_death\",\"days_to_last_follow_up\",\"case_submitter_id\"])\n",
    "protein_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df = merged_df[protein_columns].describe()\n",
    "row_means = describe_df.loc['mean']\n",
    "\n",
    "# Plot row means\n",
    "plt.figure(figsize=(10, 6))\n",
    "row_means.plot(kind='bar', color='skyblue')\n",
    "plt.title('Feature Means')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Mean')\n",
    "\n",
    "plt.xticks(range(0, len(row_means), 10), row_means.index[::10], rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop([\"ajcc_pathologic_stage\",\"vital_status\",\"days_to_last_follow_up\",\"case_submitter_id\"], axis=1)\n",
    "other = merged_df.columns.drop(\"days_to_death\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "merged_df[other] = scaler.fit_transform(merged_df[other])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_df[other], merged_df[\"days_to_death\"],\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.hist(bins=30) \n",
    "plt.xlabel('OS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the OS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE():\n",
    "    def __init__(self,X_train,X_test,y_train,y_test,bottleneck,size,type):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test        \n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.bottleneck = bottleneck\n",
    "        self.history = None\n",
    "        self.encoder = None\n",
    "        self.autoencoder = None\n",
    "        self.size = size\n",
    "        self.classifer = None\n",
    "        self.cv_scores = {}\n",
    "        self.type = type\n",
    "\n",
    "    def train(self):\n",
    "    # Number of features in your dataset\n",
    "        n_features = len(self.X_train.columns) \n",
    "\n",
    "\n",
    "        input_layer = Input(shape=(n_features,))\n",
    "        encoder = Dense(64, activation='relu')(input_layer)\n",
    "        encoder = Dense(32, activation='relu')(encoder)\n",
    "\n",
    "\n",
    "        bottleneck = Dense(self.bottleneck, activation='relu')(encoder)  \n",
    "\n",
    "        # Define the decoder (mirror the encoder)\n",
    "        decoder = Dense(32, activation='relu')(bottleneck)\n",
    "        decoder = Dense(64, activation='relu')(decoder)\n",
    "        self.encoder= Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(n_features, activation='sigmoid')(decoder) \n",
    "\n",
    "        # Define the autoencoder model\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "        self.checkpoint = ModelCheckpoint(f'model/{self.type}_{self.size}_best_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,           \n",
    "                             save_best_only=True, \n",
    "                             mode='min')         \n",
    "\n",
    "        X_train, X_test= train_test_split(self.X_train,\n",
    "                                            train_size=0.8,\n",
    "                                            random_state=1)\n",
    "\n",
    "        self.history = self.autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[self.checkpoint])  \n",
    "        \n",
    "        self.encode()\n",
    "        self.map_y()\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss '+ self.size)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def encode(self):\n",
    "        self.autoencoder.load_weights(f'model/{self.type}_{self.size}_best_model.h5')\n",
    "        self.encoded_X_train = self.encoder.predict(self.X_train)\n",
    "        self.encoded_X_test = self.encoder.predict(self.X_test)\n",
    "    \n",
    "    def do_PCA(self,n_components):\n",
    "        if self.bottleneck == 2:\n",
    "            # pca = PCA(n_components=n_components)\n",
    "            # reduced_data = pca.fit_transform(self.encoded_X_test)\n",
    "            x = self.encoded_X_test[:, 0]\n",
    "            y = self.encoded_X_test[:, 1]\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            scatter = plt.scatter(x, y, c=self.y_test_in_bin, cmap='viridis', alpha=0.7)\n",
    "            plt.title('Encoded Data '+self.size)\n",
    "            plt.xlabel('Encoded Dim 0')\n",
    "            plt.ylabel('Encoded Dim 1')\n",
    "            plt.colorbar(scatter, label='OS')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            return\n",
    "        if n_components ==2:\n",
    "            pca = PCA(n_components=n_components)\n",
    "            reduced_data = pca.fit_transform(self.encoded_X_test)\n",
    "            x = reduced_data[:, 0]\n",
    "            y = reduced_data[:, 1]\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            scatter = plt.scatter(x, y, c=self.y_test_in_bin, cmap='viridis', alpha=0.7)\n",
    "            plt.title('PCA of Encoded Data '+self.size)\n",
    "            plt.xlabel('Principal Component 1')\n",
    "            plt.ylabel('Principal Component 2')\n",
    "            plt.colorbar(scatter, label='OS')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        elif n_components ==3:\n",
    "            pca = PCA(n_components=3)  # Reduce to 3 dimensions\n",
    "            reduced_data = pca.fit_transform(self.encoded_X_test)\n",
    "            x = reduced_data[:, 0]\n",
    "            y = reduced_data[:, 1]\n",
    "            z = reduced_data[:, 2]\n",
    "            fig =plt.figure(figsize=(8, 6))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            scatter = ax.scatter(x, y, z, c=self.y_test_in_bin, cmap='viridis', depthshade=True)\n",
    "            ax.set_title('3D PCA of Encoded Data '+self.size)\n",
    "            ax.set_xlabel('Principal Component 1')\n",
    "            ax.set_ylabel('Principal Component 2')\n",
    "            ax.set_zlabel('Principal Component 3')\n",
    "            plt.colorbar(scatter, label='OS')\n",
    "            plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def map_years_to_group(value):\n",
    "        years = value / 365\n",
    "        if years <= 1:\n",
    "            return 0\n",
    "        elif 1 < years <= 3:\n",
    "            return 1\n",
    "        elif 3 < years <= 5:\n",
    "            return 2\n",
    "        elif 5 < years <= 10:\n",
    "            return 3\n",
    "        elif 10 < years <= 20:\n",
    "            return 4\n",
    "        else:  \n",
    "            return 5\n",
    "        \n",
    "    @staticmethod        \n",
    "    def map_to_binary(category):\n",
    "        if category >= 4:\n",
    "            return 1\n",
    "        else:  \n",
    "            return 0\n",
    "        \n",
    "    def map_y(self):\n",
    "        self.y_trian_in_category = self.y_train.map(AE.map_years_to_group)\n",
    "        self.y_test_in_category = self.y_test.map(AE.map_years_to_group)        \n",
    "        \n",
    "        self.y_trian_in_bin = self.y_trian_in_category.map(AE.map_to_binary)\n",
    "        self.y_test_in_bin = self.y_test_in_category.map(AE.map_to_binary)\n",
    "\n",
    "    def cross_validation_model_selection(self,fold=10):\n",
    "        classifiers = {\n",
    "            'LogisticRegression': LogisticRegression(),\n",
    "            'SVM': SVC(),\n",
    "            'RandomForest': RandomForestClassifier(),\n",
    "            'KNN': KNeighborsClassifier(),\n",
    "            'GradientBoosting': GradientBoostingClassifier(),\n",
    "            'AdaBoost': AdaBoostClassifier(),\n",
    "            'NaiveBayes': GaussianNB(),\n",
    "            'DecisionTree': DecisionTreeClassifier(),\n",
    "            'ExtraTrees': ExtraTreesClassifier(),\n",
    "            'XGBoost': xgb.XGBClassifier()\n",
    "        }\n",
    "\n",
    "        kf = KFold(n_splits=fold)\n",
    "        best_cv_score = 0\n",
    "\n",
    "\n",
    "        for name, clf in classifiers.items():\n",
    "            cv_scores = []\n",
    "            confusion_matrices = []\n",
    "\n",
    "            for train_index, test_index in kf.split(self.encoded_X_train):\n",
    "                X_train, X_test = self.encoded_X_train[train_index], self.encoded_X_train[test_index]\n",
    "                y_train, y_test = self.y_trian_in_bin.iloc[train_index], self.y_trian_in_bin.iloc[test_index]\n",
    "\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                \n",
    "                cv_scores.append(accuracy_score(y_test, y_pred))\n",
    "                confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "            mean_cv_score = np.mean(cv_scores)\n",
    "            mean_conf_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "            self.cv_scores[name] = mean_cv_score\n",
    "            \n",
    "            print(f\"{name} - Mean CV Score: {mean_cv_score}\")\n",
    "            print(f\"{name} - Mean Confusion Matrix:\\n{mean_conf_matrix}\")\n",
    "\n",
    "            if mean_cv_score > best_cv_score:\n",
    "                best_cv_score = mean_cv_score\n",
    "                best_classifier = name\n",
    "\n",
    "        print(f\"Size: {self.size}, Best classifier: {best_classifier}, CV Score: {best_cv_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    def cross_validation_hyperparameter_optimization(self,fold=5):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def do_RF(self,binary):\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "        if binary:\n",
    "            clf.fit(self.encoded_X_train, self.y_trian_in_bin)\n",
    "            y_pred = clf.predict(self.encoded_X_test)\n",
    "            cm = confusion_matrix(self.y_test_in_bin, y_pred)\n",
    "            print(classification_report(self.y_test_in_bin, y_pred))\n",
    "\n",
    "        else:\n",
    "            clf.fit(self.encoded_X_train, self.y_trian_in_category)\n",
    "            y_pred = clf.predict(self.encoded_X_test)\n",
    "            cm = confusion_matrix(self.y_test_in_category, y_pred)\n",
    "            print(classification_report(self.y_test_in_category, y_pred))\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_pred), yticklabels=np.unique(self.y_test_in_bin))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('RF Confusion Matrix '+self.type)\n",
    "        plt.show()\n",
    "        self.classifer = clf\n",
    "\n",
    "    def do_Kmean(self):\n",
    "        # Number of clusters - assuming you want as many as your known classes\n",
    "        num_clusters = 2\n",
    "        # Perform K-means clustering on the PCA output\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "        cluster_labels = kmeans.fit_predict(self.encoded_X_test)  # Use your 2D or 3D PCA-reduced data here\n",
    "\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(self.y_test_in_bin, cluster_labels))\n",
    "        conf_mat = confusion_matrix(self.y_test_in_bin, cluster_labels)\n",
    "\n",
    "        # Plotting the confusion matrix\n",
    "        sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_clusters), yticklabels=np.unique(self.y_test_in_bin))\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('K-mean Confusion Matrix '+self.size)\n",
    "        plt.show()\n",
    "        self.classifer = kmeans\n",
    "\n",
    "    def do_SVM(self,binary):\n",
    "        svm_classifier = SVC(kernel='linear',random_state=0)\n",
    "        # Load the best weights into the autoencoder model\n",
    "        if binary:\n",
    "            svm_classifier.fit(self.encoded_X_train, self.y_trian_in_bin)\n",
    "            y_pred = svm_classifier.predict(self.encoded_X_test)\n",
    "            cm = confusion_matrix(self.y_test_in_bin, y_pred)\n",
    "            print(classification_report(self.y_test_in_bin, y_pred))\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_pred), yticklabels=np.unique(self.y_test_in_bin))\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title('SVM Confusion Matrix '+ self.size)\n",
    "            plt.show()\n",
    "        else:\n",
    "            svm_classifier.fit(self.encoded_X_train, self.y_trian_in_category)\n",
    "            y_pred = svm_classifier.predict(self.encoded_X_test)\n",
    "            cm = confusion_matrix(self.y_test_in_category, y_pred)\n",
    "            print(classification_report(self.y_test_in_category, y_pred))\n",
    "        \n",
    "            # Plotting the confusion matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_pred), yticklabels=np.unique(self.y_test_in_category))\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title('SVM Confusion Matrix '+ self.size)\n",
    "            plt.show()\n",
    "        self.classifer = svm_classifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WAE(AE):\n",
    "    def train(self):\n",
    "    # Number of features in your dataset\n",
    "\n",
    "    # Number of features in your dataset\n",
    "        n_features = len(self.X_train.columns) \n",
    "\n",
    "\n",
    "        input_layer = Input(shape=(n_features,))\n",
    "        encoder = Dense(256, activation='relu')(input_layer)\n",
    "        encoder = Dense(128, activation='relu')(encoder)\n",
    "\n",
    "\n",
    "        bottleneck = Dense(self.bottleneck, activation='relu')(encoder)  \n",
    "\n",
    "        # Define the decoder (mirror the encoder)\n",
    "        decoder = Dense(128, activation='relu')(bottleneck)\n",
    "        decoder = Dense(256, activation='relu')(decoder)\n",
    "        self.encoder= Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(n_features, activation='sigmoid')(decoder) \n",
    "\n",
    "        # Define the autoencoder model\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "        self.checkpoint = ModelCheckpoint(f'model/{self.type}_{self.size}_best_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,           \n",
    "                             save_best_only=True, \n",
    "                             mode='min')         \n",
    "\n",
    "        X_train, X_test= train_test_split(self.X_train,\n",
    "                                            train_size=0.8,\n",
    "                                            random_state=1)\n",
    "\n",
    "        self.history = self.autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[self.checkpoint])  \n",
    "        \n",
    "        self.encode()\n",
    "        self.map_y()\n",
    "        \n",
    "        self.encode()\n",
    "        self.map_y()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE(AE):\n",
    "    def train(self):\n",
    "    # Number of features in your dataset\n",
    "        n_features = len(self.X_train.columns) \n",
    "    # Number of features in your dataset\n",
    "        input_layer = Input(shape=(n_features,))\n",
    "        encoder = Dense(128, activation='relu')(input_layer)\n",
    "\n",
    "        encoder = Dense(64, activation='relu')(encoder)\n",
    "        encoder = Dense(32, activation='relu')(encoder)\n",
    "\n",
    "\n",
    "        bottleneck = Dense(self.bottleneck, activation='relu')(encoder)  \n",
    "\n",
    "        # Define the decoder (mirror the encoder)\n",
    "        decoder = Dense(32, activation='relu')(bottleneck)\n",
    "        decoder = Dense(64, activation='relu')(decoder)\n",
    "        decoder = Dense(128, activation='relu')(decoder)\n",
    "\n",
    "        self.encoder= Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(n_features, activation='sigmoid')(decoder) \n",
    "\n",
    "        # Define the autoencoder model\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "        self.checkpoint = ModelCheckpoint(f'model/{self.type}_{self.size}_best_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,           \n",
    "                             save_best_only=True, \n",
    "                             mode='min')         \n",
    "\n",
    "        X_train, X_test= train_test_split(self.X_train,\n",
    "                                            train_size=0.8,\n",
    "                                            random_state=1)\n",
    "\n",
    "        self.history = self.autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[self.checkpoint])  \n",
    "        \n",
    "        self.encode()\n",
    "        self.map_y()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WDAE(AE):\n",
    "    def train(self):\n",
    "    # Number of features in your dataset\n",
    "        n_features = len(self.X_train.columns) \n",
    "\n",
    "\n",
    "        input_layer = Input(shape=(n_features,))\n",
    "        encoder = Dense(256, activation='relu')(input_layer)\n",
    "        encoder = Dense(128, activation='relu')(encoder)\n",
    "        encoder = Dense(64, activation='relu')(encoder)\n",
    "        encoder = Dense(32, activation='relu')(encoder)\n",
    "\n",
    "\n",
    "        bottleneck = Dense(self.bottleneck, activation='relu')(encoder)  \n",
    "\n",
    "        # Define the decoder (mirror the encoder)\n",
    "        decoder = Dense(32, activation='relu')(bottleneck)\n",
    "        decoder = Dense(64, activation='relu')(decoder)\n",
    "        decoder = Dense(128, activation='relu')(decoder)\n",
    "        decoder = Dense(256, activation='relu')(decoder)\n",
    "\n",
    "        self.encoder= Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(n_features, activation='sigmoid')(decoder) \n",
    "\n",
    "        # Define the autoencoder model\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer='SGD', loss='mse')\n",
    "\n",
    "        self.checkpoint = ModelCheckpoint(f'model/{self.type}_{self.size}_best_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,           \n",
    "                             save_best_only=True, \n",
    "                             mode='min')         \n",
    "\n",
    "        X_train, X_test= train_test_split(self.X_train,\n",
    "                                            train_size=0.8,\n",
    "                                            random_state=1)\n",
    "\n",
    "        self.history = self.autoencoder.fit(X_train, X_train,\n",
    "                epochs=2000,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[self.checkpoint])  \n",
    "        \n",
    "        self.encode()\n",
    "        self.map_y()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(AE):\n",
    "    def train(self):\n",
    "\n",
    "        n_features = len(self.X_train.columns)\n",
    "\n",
    "        # Define the encoder\n",
    "        input_layer = Input(shape=(n_features,))\n",
    "        # Add L1 regularization to encourage sparsity\n",
    "        encoder = Dense(64, activation='relu', \n",
    "                        activity_regularizer=regularizers.l1(1e-6))(input_layer)  # Adjust regularization rate as needed\n",
    "        encoder = Dense(32, activation='relu', \n",
    "                        activity_regularizer=regularizers.l1(1e-6))(encoder)  # Adjust regularization rate as needed\n",
    "\n",
    "        # Define the bottleneck\n",
    "        bottleneck = Dense(self.bottleneck, activation='relu')(encoder)  \n",
    "\n",
    "        # Define the decoder (mirror the encoder)\n",
    "        decoder = Dense(32, activation='relu')(bottleneck)\n",
    "        decoder = Dense(64, activation='relu')(decoder)\n",
    "        self.encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(n_features, activation='sigmoid')(decoder) \n",
    "\n",
    "        # Define the autoencoder model\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "        # Callback to save the best model\n",
    "        self.checkpoint = ModelCheckpoint(f'model/{self.type}_{self.size}_best_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "        X_train, X_test= train_test_split(self.X_train,\n",
    "                                            train_size=0.8,\n",
    "                                            random_state=1)\n",
    "\n",
    "        self.history = self.autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[self.checkpoint])  \n",
    "        self.encode()\n",
    "        self.map_y()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWDAE(AE):\n",
    "    def train(self):\n",
    "\n",
    "        n_features = len(self.X_train.columns) \n",
    "\n",
    "\n",
    "        input_layer = Input(shape=(n_features,))\n",
    "        encoder = Dense(256, activation='relu',activity_regularizer=regularizers.l1(1e-4))(input_layer)\n",
    "        encoder = Dense(128, activation='relu',activity_regularizer=regularizers.l1(1e-4))(encoder)\n",
    "        encoder = Dense(64, activation='relu',activity_regularizer=regularizers.l1(1e-4))(encoder)\n",
    "        encoder = Dense(32, activation='relu',activity_regularizer=regularizers.l1(1e-4))(encoder)\n",
    "\n",
    "\n",
    "        bottleneck = Dense(self.bottleneck, activation='relu')(encoder)  \n",
    "\n",
    "        # Define the decoder (mirror the encoder)\n",
    "        decoder = Dense(32, activation='relu')(bottleneck)\n",
    "        decoder = Dense(64, activation='relu')(decoder)\n",
    "        decoder = Dense(128, activation='relu')(decoder)\n",
    "        decoder = Dense(256, activation='relu')(decoder)\n",
    "\n",
    "        self.encoder= Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(n_features, activation='sigmoid')(decoder) \n",
    "\n",
    "        # Define the autoencoder model\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "        self.checkpoint = ModelCheckpoint(f'model/{self.type}_{self.size}_best_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,           \n",
    "                             save_best_only=True, \n",
    "                             mode='min')         \n",
    "\n",
    "        X_train, X_test= train_test_split(self.X_train,\n",
    "                                            train_size=0.8,\n",
    "                                            random_state=1)\n",
    "\n",
    "        self.history = self.autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[self.checkpoint])  \n",
    "        \n",
    "        self.encode()\n",
    "        self.map_y()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatcher(model,type,min_bottleneck, max_bottleneck,step =1):\n",
    "    current_size = min_bottleneck\n",
    "    AEs = []\n",
    "    results = []\n",
    "    while current_size <= max_bottleneck:\n",
    "        name = f\"bottleneck_{current_size}\"\n",
    "        AEs.append(model(X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test,bottleneck = current_size,size = name,type = type))\n",
    "        current_size += step\n",
    "    for AE_to_train in AEs:\n",
    "        AE_to_train.train()\n",
    "        AE_to_train.cross_validation_model_selection()\n",
    "        results.append({f\"{AE_to_train.type} {AE_to_train.size}\": AE_to_train.cv_scores})\n",
    "    \n",
    "    output = json.dumps(results)\n",
    "    with open(f\"output/{type}_model_output.json\",\"w\") as file:\n",
    "        file.write(output)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatcher(AE,\"AE\",6,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatcher(WAE,\"WAE\",6,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatcher(DAE,\"DAE\",6,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatcher(WDAE,\"WDAE\",6,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatcher(SAE,\"SAE\",6,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatcher(SWDAE,\"SWDAE\",6,48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing the JSON files\n",
    "directory = 'output'\n",
    "\n",
    "\n",
    "# List of classifiers to consider\n",
    "classifiers = [\"LogisticRegression\", \"SVM\", \"RandomForest\", \"KNN\", \"GradientBoosting\", \n",
    "               \"AdaBoost\", \"NaiveBayes\", \"DecisionTree\", \"ExtraTrees\", \"XGBoost\"]\n",
    "\n",
    "# Function to plot data from a single JSON file\n",
    "def plot_single_file(data, title):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for classifier, bottlenecks in data.items():\n",
    "        x = sorted(bottlenecks.keys())\n",
    "        y = [bottlenecks[size] for size in x]\n",
    "        plt.plot(x, y, label=classifier)\n",
    "        max_index = y.index(max(y))\n",
    "        plt.plot(x[max_index], y[max_index], 'ro')  # Mark the highest point\n",
    "        plt.text(x[max_index], y[max_index], f'{y[max_index]:.2f}', fontsize=9, ha='right')\n",
    "    plt.xlabel('Bottleneck Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize a list to hold data for each JSON file\n",
    "all_data = []\n",
    "\n",
    "# Read and process each JSON file\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "            data = {classifier: {} for classifier in classifiers}\n",
    "            for entry in json_data:\n",
    "                for bottleneck, scores in entry.items():\n",
    "                    size = int(bottleneck.split('_')[-1])\n",
    "                    for classifier, score in scores.items():\n",
    "                        data[classifier][size] = score\n",
    "            all_data.append(data)\n",
    "            plot_single_file(data, f'Performance from {filename}')\n",
    "\n",
    "# Plotting combined graph\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, data in enumerate(all_data):\n",
    "    for classifier, bottlenecks in data.items():\n",
    "        x = sorted(bottlenecks.keys())\n",
    "        y = [bottlenecks[size] for size in x]\n",
    "        plt.plot(x, y, label=f'{classifier} (File {i+1})')\n",
    "        max_index = y.index(max(y))\n",
    "        plt.plot(x[max_index], y[max_index], 'ro')  # Mark the highest point\n",
    "        plt.text(x[max_index], y[max_index], f'{y[max_index]:.2f}', fontsize=9, ha='right')\n",
    "\n",
    "plt.xlabel('Bottleneck Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Combined Classifier Performance by Bottleneck Size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources Investigation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICGC:\n",
    "https://dcc.icgc.org/repositories?filters=%7B%22file%22:%7B%20%22projectCode%22:%7B%22is%22:%5B%22HNSC-US%22%5D%7D%7D%7D\n",
    "\n",
    " Data Type\n",
    " SSM  2,126\n",
    " Aligned Reads  2,037\n",
    " Clinical Data  453\n",
    " Biospecimen Data  448\n",
    " StSM  223\n",
    " SGV  132\n",
    " CNSM  88\n",
    " StGV  88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCPA:\n",
    "https://www.tcpaportal.org/tcpa/download.html\n",
    "\n",
    "TCGA of 2018, with L4(normalized across RPPA batches therefore enable pan-cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDC:\n",
    "https://proteomic.datacommons.cancer.gov/pdc/browse\n",
    "3 studies, but Mass Spectrum not RPPA, therefore only contains Peptide result. do have clinincal though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HNSCC PDX: \n",
    "https://aacrjournals.org/mcr/article/14/3/278/89624/Proteomic-Characterization-of-Head-and-Neck-Cancer\n",
    "RPPA, but on mention how to acess and probabaly wound not have clinical since the read from transplated rats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAP: Reference RNA and protein from healthy samples:\n",
    "https://www.proteinatlas.org/about/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pride:Full MS sets\n",
    "https://www.ebi.ac.uk/pride/archive?keyword=HNSCC,RPPA&sortDirection=DESC&page=0&pageSize=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper HNSCC: RPPA but only target 60 specific protein\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3070553/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEO: Some Protein profiling by protein array (RPPA), no HNSCC\n",
    "https://www.ncbi.nlm.nih.gov/geo/browse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArraryExpress: RPPA for GBM, lung cancer, breast cancer\n",
    "https://www.ebi.ac.uk/biostudies/arrayexpress/studies?query=RPPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FANTOM6 Experiment Index: RNA-Seq\n",
    "https://fantom.gsc.riken.jp/6/experiment_index/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources index: \n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6971871/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
